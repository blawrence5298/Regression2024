---
title: "Final_Project_Paper"
author: "Ben Lawrence"
format: pdf
editor: visual
---

```{r setup }
suppressMessages(source("setup.R"))
```

::: {align="center"}
## Introduction
:::

In this paper we investigate the question of which student attributes affect student academic performance. The adage "Skip class you won't pass?" presents us with what might appear to be an obvious relationship. If a student does not show up to learn their class material, they will inevitably struggle when it comes time to evaluate their learning. Many state and national governments have adopted policies to ensure that students "stay in school", and parents who allow their children to be absent for enough days are punished. Clearly there is a general belief that keeping absences low will generally benefit the academic performance of students and in general benefit society.

Some educational researchers believe that there is a strong effect that failing a class has on subsequent student performance. If a student fails one class, they may fall into a pattern where they are psychologically influenced to perceive their previous effort as wasted. In the pressure to catch up in the following grading period, students may lose the persistence and self-efficacy needed to succeed. In this way, failure can compound and become a predictor of student performance. Research by Rola Ajjawi et. al studied student attitudes and performance after failing college units and found their figures "indicate a significant contribution of academic failure to drop out".

There appears to be general consensus that prior academic grades are one of the best predictors of future academic grades of students (Alyahyan, Düştegör, 2020). However, some challenge the validity of using prior performance to predict future performance, because when data from prior academic contexts is used in can result in inaccurate predictions. For example, if a student performed poorly in high school math, that grade would likely not be a useful predictor of their success in a college level Literature class. The difference in subject and grade level takes the student's performance out of its appropriate context. A better predictor would be the student's grade in a prerequisite English or other Literature class. When proper context is considered prior grades are a strong predictor of future grades, but some also question whether grades are an accurate measure of student success. We will discuss this further in the conclusion. In the following section we shall discuss the dataset we used and conduct regression analyses to examine the predictors discussed above.

::: {align="center"}
## Data Description
:::

We obtained our two datasets from the UC Irvine Machine Learning Repository (UCIMLR). These datasets were used in the article "Using Data Mining to Predict Secondary School Student Performance" by Paulo Cortez and Alice Silva from the University of Minho, Portugal. They contain student grade and background information for two core classes in the Portuguese curriculum, Portuguese and Math. The Math dataset is a 395 x 33 dataset, and the Portuguese is a 649 x 33 dataset. They were donated to the UCIMLR in 2014.

In this article, the dataset was used to train five models (naive predictor, neural network, support vector machine, decision tree, and random forest) to predict three different attributes of the data. The first attribute is a binary pass/fail classification based on a student's final grade. The second is a 5-level classification based on the Erasmus grade conversion system which ranks students from 1 to 5 based on bins of their final grade. (Table 2 in the article) The third attribute is a regression output of the final student grade, which is numeric between 0 and 20.

The main conclusion of the article is the grade of the student in the prior grading period (when available) is the most important predictor across the best performing models. When the immediate prior grade is unavailable the next prior grade becomes the most important, and when both are unavailable, other predictors such as number of past class failures, and number of absences become important.

The Portuguese school system operates on a trimester model where three distinct grading periods take place and the last trimester grade is taken as the student's final grade. We shall attempt to predict the same variable as the paper "*G3*" (the student's grade for the third trimester and final grade) using simple linear regression (SLR) and multiple linear regression (MLR) techniques. We will begin our analysis by creating simple plots for the variables that the paper identified as important. These include prior trimester grades, number of absences, and past class failures.

Here we transform our data to be suitable for SLR and MlR. We transform variables recorded on an ordinal scale (rate your health from 1 to 5), as factors. For descriptions of each row of the datasets, please see the Code Appendix below.

```{r}
math_perf <- read.csv("student-mat.csv", sep=";")
port_perf <- read.csv("student-por.csv", sep=";")

# Factor these data columns
fcols <- c('school', 'sex', 'age', 'address', 'famsize', 'Pstatus', 'Medu', 'Fedu', 'Mjob', 'Fjob', 'traveltime', 'studytime', 'reason', 'guardian', 'schoolsup', 'famsup', 'paid', 'failures', 'activities', 'nursery', 'higher', 'internet', 'romantic', 'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health')

# Factorize columns in both dataframes
math_perf[fcols] <- lapply(math_perf[fcols], factor)
port_perf[fcols] <- lapply(port_perf[fcols], factor)  # Corrected this line

# Transform absences - Log transformation to handle skewness
math_perf$absence_pow <- math_perf$absences^(0.3)
port_perf$absence_pow <- port_perf$absences^(0.3)



#head(math_perf)
#head(port_perf)
```

**Figure 1** below shows a scatter plot of *G3* vs. *G2* and *G3* vs. *G1* for both the Math and Portuguese classes. As we can see the relationship between them is roughly linear for all plots, meaning that *G2* and *G1* are both likely strong predictors in a linear regression model. This is consistent with other models presented in the article.

**Figure 1**

```{r}
p1 <- gg_basic(math_perf,G2,G3)
p2 <- gg_basic(port_perf,G2,G3)
p3 <- gg_basic(math_perf,G1,G3)
p4 <- gg_basic(port_perf,G1,G3)

grid.arrange(p1, p2,p3, p4, ncol=2, nrow=2)
```

The article suggests that absences is also an important predictor. In **Figure 2** we have plotted *absences* vs. *G3* and *failures* vs. *G3* and we can see a negative correlation, but it is very slight. As we might expect, students with more absences do not perform at a high level, but we can see students who have very few absences perform both very well and very poorly. For failures, the number of failures does influence the *G3* value, but because failures is a very discrete variable (takes values 1, 2 ,3, 4), it is hard to establish a continuous linear relationship between it and *G3*.

**Figure 2**

```{r}

p5 <- gg_basic(math_perf,absences,G3, title = "Mathematics")
p6 <- gg_basic(port_perf,absences,G3)
p7 <- gg_basic(math_perf,failures,G3)
p8 <- gg_basic(port_perf,failures,G3)

grid.arrange(p5, p6, p7, p8, ncol=2, nrow=2)
```

This dataset contains many other variables that may affect student performance as measured by *G3.* Although given the visually weak linear relationship between variables other than prior trimester grades, it may be that the rest of the variables are weak as well or are irrelevant inputs. We shall still investigate these in a complete MLR model later.

::: {align="center"}
## Methods and Results (Analysis)
:::

First we will construct SLR models for the variables we have plotted above to confirm our visual intuition. We shall also create diagnostic plots to judge model validity.

In **Figure 3**, we can see *G2* is a highly significant predictor in our models for both clases. However when we look at the diagnostic plots we can see two patterns in the fitted vs. residuals. There is a slight linear relationship with the majority of points and a clear linear relationship with the outlying lower grades. This indicates the variance between the majority grade levels and the lower levels is not constant, calling into question our model validity. The QQ plot and Leverage plot indicate that majority higher grade levels are normal, but the lower levels are not, and abnormally high and low grades tend to have high leverage in our model.

Overall, although the prior trimester grade does not seem appropriate as the sole predictor of the final grade in a linear model. This result holds with *G1* as well.

**Figure 3**

```{r}
lm1 <- lm(G3 ~ G2, data = math_perf)
summary(lm1)$coefficients
lm_diag(lm1)

summary(powerTransform(lm1))

lm2 <- lm(G3 ~ G2, data = port_perf)
summary(lm2)$coefficients
lm_diag(lm2)

#lm3 <- lm(G3 ~ G1, data = math_perf)
#summary(lm3)$coefficients
#lm_diag(lm3)

#lm4 <- lm(G3 ~ G1, data = port_perf)
#summary(lm4)$coefficients
#lm_diag(lm4)
```

We may observe the results of SLR models of the remaining predictors that we have discussed. We can see that the $R^2$ value is extremely low for these predictors. Even if this model was valid, its predictive power would be quite low, so we will omit the diagnostics for the sake of brevity.

```{r}
lm5 <- lm(G3 ~ absences, data=math_perf)
summary(lm5)$coefficients
lm_diag(lm5)

lm6 <- lm(G3 ~ absences, data=port_perf)
summary(lm6)$coefficients
lm_diag(lm6)
```

```{r}

lm6$model$G3 <- lm6$model$G3 + 1
```

```{r}
lm5$model$G3 <- lm5$model$G3 + 1
```

```{r}
summary(powerTransform(lm6))
```

```{r}
lm7 <- lm(G3 ~ failures, data=math_perf)
summary(lm7)$coefficients
lm8 <- lm(G3 ~ failures, data=port_perf)
summary(lm8)$coefficients

lm_diag(lm7)
lm_diag(lm8)
```

Clearly SLR is not an appropriate technique for creating a valid or useful model and we must expand to a MLR model that will incorporate multiple predictors. We shall begin with a full model.

```{r}
lm_full_math <- lm(G3 ~ . , data=math_perf)
#Omitted for brevity
#summary(lm_full_math)

lm_full_port <- lm(G3 ~ ., data=port_perf)
#Omitted for brevity
#summary(lm_full_port)
```

The full model contains many insignificant predictors that should be filtered out. In **Figure 4** we can see the diagnostics of the full models appear similar to the SLR model with the prior grades as predictors.

**Figure 4**

```{r}
lm_diag(lm_full_math)
lm_diag(lm_full_port)
```

First we will cull our models to obtain only significant predictors using the step function.

```{r}

#Commented Out for Brevity
#lm9 <- step(lm_full_math)

#Significant predictors
#schoolsup, romantic, health
#activities, famrel, failures
#G1, absences, G2

#Commented Out for Brevity
#lm10 <- step(lm_full_port)

#Significant predictors
#goout, address, reason
#Dalc, sex, traveltime
#failures, absences, G1, G2

```

Above we have recorded the significant predictors found using the best AIC values for both datasets. It is interesting to note some differences between the two, and they both share *G1*, *G2*, *absences*, and *failures* as significant predictors. We shall create MLR models using these predictors and continue to cull based on significance values.

```{r}
lm11 <- lm(G3 ~ schoolsup + romantic + health + activities + famrel + failures + G1 + absences + G2, data = math_perf)
summary(lm11)

lm12 <- lm(G3 ~ goout + address + reason + Dalc + sex + traveltime + failures + absences + G1 + G2, data = port_perf)
summary(lm12)
```

Based on the significance values above we will cull any predictor with p-value more than 0.05. If a categorical variable value has significance less than 0.05, all values of that variable will be included.

```{r}
lm13 <- lm(G3 ~ activities + failures + absences + G1 + G2, data = math_perf)
summary(lm13)

lm14 <- lm(G3 ~ reason + Dalc + failures + absences + G1 + G2, data = port_perf)
summary(lm13)
```

In Figure 5 we examine the diagnostics of both models.

**Figure 5**

```{r}
lm_diag(lm13)
lm_diag(lm14)
```

We still have similar problems with the residuals that we had with prior models. We shall investigate if there is multicollinearity present among the predictors by creating a correlation matrix. We can see that *G1* and *G2* are highly correlated so it would appropriate to only include 1 in our next models.

```{r}
cor_matrix(lm13)
```

The models below are likely the "best" we will obtain. The problem with a pattern in the residuals is still present, however in the majority of fitted values there is not a visually obvious pattern. There is a set of outliers that have clearly linear residuals. This may indicate there are a subset of students (who generally get graded lower) which our models may not adequately address. That being said, the Leverage plots indicate few outliers that are also high leverage, providing evidence our models are sound for the average student. The QQ plot further supports this idea, given that the lower outliers fall short of the normal line.

```{r}
pairs(G3 ~ G2 + failures + absences + activities, data = math_perf)
```

```{r}

# Define the model using the new variable
lm15 <- lm(G3 ~ activities + failures + absence_pow + G2, data = math_perf)
summary(lm15)#$coefficients
lm_diag(lm15)  # Assuming lm_diag is a function you've defined for diagnostics


lm16 <- lm(G3 ~ reason + Dalc + failures + absences_pow + G2, data = port_perf)
summary(lm16)#$coefficients
lm_diag(lm16)
```

```{r}
math_perf$avg_grade <- (math_perf$G1 + math_perf$G2 + math_perf$G3) / 3
port_perf$avg_grade <- (port_perf$G1 + port_perf$G2 + port_perf$G3) / 3

# Define the model using the new variable
lm15 <- lm(avg_grade ~ failures + absences^(0.3) + G2, data = math_perf)
summary(lm15)#$coefficients
lm_diag(lm15)  # Assuming lm_diag is a function you've defined for diagnostics


lm16 <- lm(avg_grade ~ failures + absences^(0.3) + G2, data = port_perf)
summary(lm16)#$coefficients
lm_diag(lm16)
```

```{r}
head(math_perf)
math_perf$Pass <- math_perf$G3 >= 14
math_perf$Pass <- as.numeric(math_perf$Pass)

log1 <- glm(Pass ~ . , data=math_perf, family="binomial")

p1 <- ggplot(math_perf, aes(G2, Pass)) + geom_point() + geom_smooth(method="glm", method.args = list(family = "binomial"), se=F)

ggplotly(p1)
```

```{r}
probs <- predict(log1, newdata = math_perf, type = "response")
roc_obj <- roc(math_perf$G3, probs)
```

```{r}
plot(1 - roc_obj$specificities, roc_obj$sensitivities, type="l",
     xlab="1 - Specificity", ylab = "Sensitivity")

abline(0,1,lty=2)
```

::: {align="center"}
## Conclusion
:::

Our models largely agree with the models in the original article. For both classes, the most significant predictors are *G1*, *G2*, *absences*, and *failures*. For our models each class had one additional significant predictor different from the other, *activities* for Math and *reason* for Portuguese. Since these predictors are not highly significant with their respective datasets, it is likely this is not suggestive of a difference between performance in Math and Portuguese and they both may be discarded in pursuit of a more general model of student performance without much concern.

Our results and the results of the article suggest several interesting questions of which interventions may be effective in increasing student performance. Since number of class failures and number of student absences are significant across models and class subjects, these would be useful starting points for interventions. Strategies which reduce student absenteeism and allow students additional opportunities (extra credit, etc.) to avoid class failure could improve student performance.

However it should be noted that when considered individually as SLR models, these predictors offered very little predictive power according to their $R^2$ values. This could be a result of the how the data was formatted in the two datasets. Failures, in particular was partitioned into 4 discrete values, making it a poor candidate to have a continuous linear relationship with *G3.* The classification and Random Forest models in the article may be better ways to assess the importance of these two predictors than SLR or MLR. All models agree however, when present, *G2* is the most significant predictor of *G3* (Cortez, Silva, 2008).

This suggests that prior student grades are strong predictors of future grades. This is not particularly useful information when designing academic interventions because interventions that improve grades should perform the same across time in the same class. It also may suggest that academic performance is a skill that is independent of other sociological factors attributed to students. Student performance may not be as closely tied to factors that we reasonably suspect influence academic ability (such as support outside of class). Professors who experience a spike in office hour attendance near the end of the semester likely have suspected as much.

There has been debate in the educational community on whether grades are an accurate measure of educational achievement. Studies have shown that secondary student grades do not always correlate with other measures of achievement like standardized test scores (Pollio, Hochbein, 2015). Teachers have reported giving grade credit for nonacademic factors such as participation, behavior, and attitude. Moreover, teachers often differentiate assignments between students based on academic aptitude. Students who are high performing receive assignments tailored to the lesson content, while low performing students may receive assignments tailored to cover material in the previous grade level to remediate perceived deficiencies. Both groups would receive the same amount of grade credit for completing the assignment, yet the low performing group had not achieved mastery of the lesson content. For any concerned parties, it should follow that interventions designed to improve student grades may not improve actual content mastery.

Care should be taken to examine the complex array of sociological factors and nuances involved in promoting the academic success of students. The datasets used in this paper are from a particular education level from a particular country. Conclusions derived from this data should bear in mind that there is great variation between teachers, grade levels, schools, and nations in how student performance is evaluated. This presents a problem in generalizing conclusions from this dataset outside of a Portuguese secondary education context and without an expert sociological analysis of our results.

In conclusion we present some additional questions for further research.

-   Do the conclusions of our models hold in a Portuguese post-secondary context? Primary?

-   Do the conclusions hold in analyses of the American education system? British? Chinese? What effects do cultural evaluations of student performance have?

-   What sociological factors not present in our dataset reasonably affect student performance or academic ability? Are they highly correlated with prior student grades?

-   Are there superior models to SR, MLR, Decision Trees, or Random Forests when predicting student performance?

Our findings help frame the dynamic topic of improving student educational performance and underscore the importance of making data driven decisions in education while keeping broader cultural and sociological factors in mind.

::: {align="center"}
## Code Appendix
:::

```{r}
#Relevant libraries
library(ggplot2)
library(plotly)
library(grid)
library(gridExtra)

#Custom functions to avoid redundant coding
#Creates basic scatter plots
gg_basic <- function(data, x, y) {
  plot <- ggplot(data = data, aes(x = {{x}}, y = {{y}})) + 
          geom_point()
  return(plot)
}

#Creates a 2x2 grid of basic diagnostic plots of a lm object
lm_diag <- function(lm_model) {
  p1 <- gg_basic(lm_model$model, fitted(lm_model), residuals(lm_model)) + 
    labs(x="Fitted Values", y="Residuals")
  p2 <- gg_basic(lm_model$model, fitted(lm_model), rstandard(lm_model)) +
    labs(x="Fitted Values", y="Standardized Residuals")
  p3 <- gg_basic(lm_model$model, fitted(lm_model), hatvalues(lm_model)) +
    labs(x="Fitted Values", y="Leverage")
  
  # Create QQ plot of standardized residuals
  p4 <- ggplot(lm_model$model, aes(sample = rstandard(lm_model))) +
        stat_qq() +
        stat_qq_line(colour="red") + 
        labs(x ="Theoretical Quantiles", y="Sample Quantiles") +
        ggtitle("QQ Plot of Standardized Residuals")

  # Arrange all plots into a grid
  return(grid.arrange(p1, p2, p3, p4, ncol=2, nrow=2))
}


#Creates a correlation matrix for predictors in a lm model
cor_matrix_from_lm <- function(lm_model) {
  model_data <- lm_model$model
  
  # Get names of all variables from the model formula
  all_vars <- all.vars(formula(lm_model))
  
  # Get names of predictors from the model formula (exclude the response variable)
  predictors <- all_vars[all_vars != response_var]
  pred_data <- model_data[predictors]
  
  # Attempt to convert factors to numeric if not already
  for(col in colnames(pred_data)) {
    if(is.factor(pred_data[[col]])) {
      pred_data[[col]] <- as.numeric(as.factor(pred_data[[col]]))
    }
  }
  
  cor_matrix <- cor(pred_data, use = "complete.obs")  # using complete cases
  return(cor_matrix)
}

```

::: {align="center"}
## References
:::

Ajjawi, R., Dracup, M., Zacharias, N., Bennett, S., & Boud, D. (2020). Persisting students’ explanations of and emotional responses to academic failure. *Higher Education Research & Development*, *39*(2), 185–199. https://doi.org/10.1080/07294360.2019.1664999

Alyahyan, E., Düştegör, D. Predicting academic success in higher education: literature review and best practices. *Int J Educ Technol High Educ* **17**, 3 (2020). https://doi.org/10.1186/s41239-020-0177-7

Cortez, P., & Silva, A.M. (2008). Using data mining to predict secondary school student performance.

Pollio, M., & Hochbein, C. (2015). The Association between Standards-Based Grading and Standardized Test Scores as an Element of a High School Reform Model. Teachers College Record, 117(11), 1-28. <https://doi.org/10.1177/016146811511701106>
