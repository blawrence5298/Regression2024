---
title: "Homework 4"
author: Ben Lawrence
output: html_document
---

# Homework 4

## Exercise 1

The multiple linear regression model can be written as $Y=X\beta+e$, where $\text{Var(e)}=\sigma^2I$ and $I$ is the $n \times n$ identity matrix. The fitted values are given by

$$
\hat Y=X\hat \beta=X(X^TX)^{-1}X^TY=HY
$$

where $H=X(X^TX)^{-1}X^T$

\(a\) Show that $HH^T=HH=H$. Note that a matrix that has this property is called idempotent.

First we note the following properties of matrices

$$
\text{Let A, B, C be invertible matrices whose product }ABC \text{ exists}.\\
 \text{Then }(ABC)^T=C^T(AB)^T =C^TB^TA^T \text{and }(A^T)^{-1}=(A^{-1})^T
$$

Using these properties we obtain

$$
\begin{aligned}
HH^T &=X(X^TX)^{-1}X^T[X(X^TX)^{-1}X^T]^T \\
&= X(X^TX)^{-1}X^T(X^T)^T[(X^TX)^{-1}]^TX^T \\
&= X(X^TX)^{-1}X^TX[(X^TX)^T]^{-1}X^T \\
&= X(X^TX)^{-1}X^TX(X^TX)^{-1}X^T=HH \checkmark \\
&= X(X^TX)^{-1}(I)X^T \\
&=X(X^TX)^{-1}X^T=H \checkmark
\end{aligned}
$$

\(b\) Show that $\rm E(\hat Y)=X\beta$

$$
\begin{aligned}
\rm E(\hat Y) &=\rm E(HY)=\rm E[H(X \beta+e)] \\
&=H[\beta X+E(e)]=H[X \beta+ \mathbf{0}] \\
&=HX \beta=X(X^TX)^{-1}X^TX\beta \\
&=XI\beta=X\beta \checkmark
\end{aligned}
$$

\(c\) Show that $\rm Var(\hat Y)=\sigma^2H$

$$
\begin{aligned}
\rm Var(\hat Y) &=\rm Var(HY)=\rm Var[H(X\beta+e)] \\
&=H \rm Var(X\beta+e) \\
&=H \rm Var(e)\\
&=H\sigma^2I \\
&=\sigma^2HI \\
&=\sigma^2H \checkmark
\end{aligned}
$$

## Exercise 2

In lecture 10 we showed the variance-covariance matrix for the $(p+1)\times1$ vector, $\hat \beta$, of least squares estimates is given by $\rm Var(\hat \beta)=\sigma^2(X^TX)^{-1}$. Derive the $2\times2$ variance-covariance matrix for least squares estimates, $\hat\beta=(\hat\beta_0,\hat\beta_1)^T$, for simple linear regression:

$$
\rm Var(\hat\beta)= 
\begin{pmatrix}
\rm Var(\hat\beta_0) & \rm Cov(\hat\beta_0,\hat\beta_1) \\
\rm Cov(\hat\beta_1,\hat\beta_0) & \rm Var(\hat\beta_1) \\
\end{pmatrix}
$$

Additionally, use your result to verify that $\rm Var(\hat\beta_0)=\sigma^2(\frac{1}{n}+\frac{\bar x^2}{SXX})$ and $\rm Var(\hat\beta_1)=\sigma^2/SXX$, where $\rm SXX=\sum_{i=1}^n(x_i-\bar x)^2$. \[Hint: it might be useful to use the identity $\sum_{i=1}^n(x_i-\bar x)^2=\sum x_i^2-n\bar x^2$\]

The $X$ matrix for a simple linear regression with $n$ points is given by

$$
X=
\begin{bmatrix} 
1 & x_1 \\
1 & x_2 \\
\vdots & \vdots \\
1 & x_n
\end{bmatrix}
$$

Then we apply the general result to obtain the variance covariance matrix

$$
\begin{aligned}
\sigma^2(X^TX)^{-1} &=\sigma^2
\left( 
\begin{bmatrix}
1  & x_1 \\
1 & x_2 \\
\vdots & \vdots \\
1 & x_n 
\end{bmatrix}
\begin{bmatrix} 
1 & 1 & \cdots & 1 \\
x_1 & x_2 & \cdots & x_n
\end{bmatrix}
\right)^{-1} \\
&=\sigma^2 \left( 
\begin{bmatrix} 
1+1+\cdots+1 & x_1+x_2+\cdots+x_n \\
x_1+x_2+\cdots+x_n & x_1^2+x_2^2+\cdots+x_n^2
\end{bmatrix}
\right)^{-1} \\
&= \sigma^2 \begin{bmatrix} 
n & \sum_{i=1}^nx_i \\
\sum_{i=1}^nx_i & \sum_{i=1}^nx_i^2
\end{bmatrix}^{-1 \\} \\
&=\frac{\sigma^2}{n} 
\begin{bmatrix} 
1 & \bar x \\
\bar x & \frac{1}{n}\sum_{i=1}^nx_i^2
\end{bmatrix}^{-1} \\
&=\sigma^2 \cdot\frac{1}{n(\frac{1}{n}\sum_{i=1}^nx_i^2-(\bar x)^2)} 
\begin{bmatrix} 
\frac{1}{n}\sum_{i=1}^nx_i^2 & -\bar x \\
-\bar x & 1
\end{bmatrix} \\
&= \sigma^2 \cdot \frac{1}{SXX} \begin{bmatrix} 
\frac{1}{n}\sum_{i=1}^nx_i^2 & -\bar x \\
-\bar x & 1
\end{bmatrix}
\end{aligned}
$$

After applying the provided identity we obtain

$$
\frac{\frac{1}{n}\sum_{i=1}^nx_i^2}{SXX}=\frac{\frac{1}{n}(SXX+n\bar x)}{SXX}=\frac{1}{n}+ \frac{\bar x}{SXX}
$$

So our variance covariance matrix becomes,

$$
\begin{bmatrix} 
\sigma^2(\frac{1}{n}+\frac{\bar x}{SXX}) & \sigma^2(\frac{-\bar x}{SXX}) \\
\sigma^2(\frac{-\bar x}{SXX}) & \frac{\sigma^2}{SXX}
\end{bmatrix}
=\begin{bmatrix} 
\rm Var(\hat\beta_0) & \rm Cov(\hat\beta_0,\hat\beta_1) \\
\rm Cov(\hat\beta_1,\hat\beta_0) & \rm Var(\hat\beta_1)
\end{bmatrix} \checkmark
$$

## Exercise 3

For this exercise use the $\rm Boston$ data set from the $\rm MASS$ package. Consider the multiple linear regression model with $\rm medv$ as the response and $\rm dis, rm, tax$ and $\rm chas$ as predictor variables.

\(a\) In R, compute the vector of least squares estimates $\hat\beta=(X^TX)^{-1}X^TY$. Then verify that the results are the same as the parameter estimates provided by the $\rm lm()$ function.

\(b\) In R, compute the variance-covariance matrix $\rm Var(\hat\beta)=\sigma^2(X^TX)^{-1}$ (plug in $\sigma^2=\rm RSS/(n-p-1)$ as the estimate for $\sigma^2$). Then verify that the square root of the diagonal entries of this matrix are the same as the standard errors provided by the the $\rm lm()$ function.

```{r}
library(MASS)
head(Boston)
```

### Part a

```{r}
#Create a matrix of predictors using the dis, rm, tax, and chas columns of the Boston dataset, including a column of 1s to account for beta_0
X <- as.matrix(cbind(1, Boston[, c('dis', 'rm', 'tax', 'chas')]))

#Create a matrix of responses
Y <- as.matrix(Boston['medv'])

#Get the transpose of the predictors matrix
X_t <- t(X)

#Apply the least-squares estimates formula
Beta <- solve(X_t %*% X) %*% X_t %*% Y
Beta
```

Now we use the $\rm lm$ function and see if our results agree

```{r}
lm1 <- lm(medv ~ dis + rm + tax + chas, data=Boston)
summary(lm1)
```

And we can observe that they do.

### Part b

```{r}
#First we obtain the factors necessary to calculate the variance of predicted values
RSS <- sum((residuals(lm1))^2)
n <- nrow(Boston)

#Calculate sigma squared manually
sigma_squared <- RSS / (n - 4 - 1)

#Now we apply the given formula to obtain the covariance matrix
Cov <- sigma_squared * solve(X_t %*% X)

#From the matrix we isolate the diagonal entries and square root them to obtain the standard errors
Err <- sqrt(diag(Cov))
Err

```

Indeed the manually calculated standard errors and errors reported by the $\rm lm$ function are the same $\checkmark$
