---
title: "Midterm 2 Prep"
---

## Midterm 2 Prep

Since this prof is obsessed with visualization we will write down every significant plot code that he has provided in lecture. Beginning with degree two polynomial regression.

```{r}
ProfSalary <- read.table("profsalary.txt", header=TRUE)

lm1 <- lm(Salary ~ Experience, data=ProfSalary)

plot(Salary ~ Experience, data=ProfSalary)

abline(lm1, col="red")

Salary = ProfSalary$Salary
Experience = ProfSalary$Experience
```

```{r}
library(ggplot2)
ggplot(data=ProfSalary, aes(Experience, Salary)) +
  geom_point() +
  stat_smooth(method='lm',formula = y ~ poly(x,2))
```

It may be better to work on some of the book exercises as a way to practice and reinforce skills. The following are numbers 2 and 3 from the end of Chapter 3 Exercises in the Textbook. pp. 116-117 (pdf count).

2.  Is the following statement true or false? If you believe that the statement is false, provide a brief explanation.

    Suppose that a straight line regression model has been fit to bivariate data set of the form $(x_1,y_1), (x_2,y_2),\dots,(x_n,y_n)$. Furthermore, suppose that the distribution of $X$ appears to be normal while the $Y$ variable is highly skewed. A plot of standardized residuals from the least squares regression line produce a quadratic pattern with increasing variance when plotted against $(x_1,x_2,\dots,x_n)$ . In this case, one should consider adding a quadratic term in $X$ to the regression model and thus consider a model of the form $Y=\beta_0+\beta_1x+\beta_2x^2+e$.

    Here we can use the first midterm data set as a counterexample.

    ```{r}
    #Load ggplot2
    library(ggplot2)

    #Read the data
    Possum <- read.csv("midterm1data.csv")
    head(Possum)

    #Define Response and Predictor
    Y <- Possum$head_l
    X <- Possum$skull_w
    ```

Let's take a look at the distribution of X and Y

```{r}
#Set up 1 row 2 columns
par(mfrow=c(1,2))

ggplot(data = Possum, aes(x = skull_w)) + 
  geom_histogram(bins = 40)
ggplot(data = Possum, aes(x = head_l)) + 
  geom_histogram(bins = 40)
```

This isn't actually going to help since it appears its the x variable that is slightly skewed. In any event its not skewed enough to provide a very convincing counterexample. I'm going to ask chat gpt to produce something more helpful here.

It came back with a python simulation. I'll try to recreate it in r here.

```{r}
library(sn)

set.seed(42)

# Generate x values from a normal distribution
x <- rnorm(n = 100, mean = 0, sd = 1)

# Ensure x is strictly positive for log(x) computation
x_adjusted <- x + abs(min(x)) + 0.1

# Create y values using adjusted x to ensure all operations are finite
alpha <- 10
y <- rsn(n = 100, xi = 0.5 * x_adjusted^2 * log(x_adjusted), omega = 0.1 + 0.1 * abs(x_adjusted), alpha = alpha)

# Identify indices of finite y values
finite_indices <- which(is.finite(y))

# Filter both x and y to ensure they align and have the same length
x_aligned <- x[finite_indices]
y_aligned <- y[finite_indices]


# Fit a linear model and calculate residuals
model <- lm(y ~ x)
residuals <- rstandard(model)

ggplot() + 
  geom_histogram(aes(x = x))

ggplot() + 
  geom_histogram(aes(x = y))

ggplot() +
  geom_point(aes(x = x, y = residuals), alpha = 0.7) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(title = "Plot of Standardized Residuals", x = "X", y = "Standardized Residuals")

```

We can see that the standardized residuals have a clear quadratic pattern. Let's verify that a quadratic model is not the best.

```{r}
quad = lm(y ~ poly(x,2))
summary(quad)
cube = lm(y~ poly(x,3))
summary(cube)
```

Okay so we have a bit of a problem where its tricky to simulate something like this. Perhaps some strange combo of functions and noise could pull off this effect, but I'm gonna move on to questions 3.

3.  The price of advertising (and hence revenue from advertising) is different from one consumer magazine to another. Publishers of consumer magazines argue that magazines that reach more readers create more value for the advertiser. Thus, circulation is an important factor that affects revenue from advertising. In this exercise, we are going to investigate the effect of circulation on gross advertising revenue. The data are for the top 70 US magazines ranked in terms of total gross advertising revenue in 2006. In particular we will develop regression models to predict gross advertising revenue per advertising page in 2006 (in thousands of dollars) from circulation (in millions). The data were obtained from http://adage.com and are given in the file AdRevenue.csv which is available on the book web site. Prepare your answers to parts A, B, and C in the form of a report.

    Part A)

    ```{r}
    #Load Ad Revenue
    ARev <- read.csv("AdRevenue.csv")
    head(ARev)
    ```

Here we create a scatter plot of the x and y variables and examine histograms to assess normality

```{r}
ggplot(data = ARev, aes(x = Circulation, y = AdRevenue)) +
  geom_point()

ggplot() +
  geom_histogram(data = ARev, aes(x = Circulation),bins = 50)

ggplot() +
  geom_histogram(data = ARev, aes(x = AdRevenue),bins = 50)
```

Okay so our
